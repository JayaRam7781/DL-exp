{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPAyma5ztc77eDD631HRf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayaRam7781/DL-exp/blob/main/External_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvZZK_Z790dR",
        "outputId": "1dd3b478-5f81-48f0-de1c-34d12975d0d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 1]]\n",
            "[[0, 0, 1], [0, 1, 0]]\n",
            "[[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
            "[[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1]]\n",
            "[[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0]]\n",
            "[[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]]\n",
            "[[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "# Define the list of words\n",
        "words = ['apple', 'banana', 'cherry', 'apple', 'cherry', 'banana', 'apple']\n",
        "# Create a vocabulary of unique words\n",
        "vocab = set(words)\n",
        "# Assign a unique integer to each word in the vocabulary\n",
        "word_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "# Convert the list of wordsto a list of integers using the vocabulary\n",
        "int_words = [word_to_int[word] for word in words]\n",
        "# Perform one-hot encoding ofthe integer sequence\n",
        "one_hot_words = []\n",
        "for int_word in int_words:\n",
        "  one_hot_word = [0] * len(vocab)\n",
        "  one_hot_word[int_word] = 1\n",
        "  one_hot_words.append(one_hot_word)\n",
        "  print(one_hot_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "# Define the input string\n",
        "input_string = 'hello world'\n",
        "# Create a vocabulary of unique characters\n",
        "vocab = set(input_string)\n",
        "# Assign a unique integer to each character in the vocabulary\n",
        "char_to_int = {char: i for i, char in enumerate(vocab)}\n",
        "# Convert the input string to a list of integers using the vocabulary\n",
        "int_chars = [char_to_int[char] for char in input_string]\n",
        "# Perform one-hot encoding ofthe integer sequence\n",
        "one_hot_chars = []\n",
        "for int_char in int_chars:\n",
        "  one_hot_char = [0] * len(vocab)\n",
        "  one_hot_char[int_char] = 1\n",
        "  one_hot_chars.append(one_hot_char)\n",
        "  print(one_hot_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSMwJ4WO-aby",
        "outputId": "ab8c3b96-694f-421c-c339-f1689a47a2d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 0, 0, 0, 0, 0, 0, 1]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dense,\n",
        "Embedding\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Getting reviews with wordsthat come under 5000\n",
        "# most occurring words in the entire\n",
        "# corpus oftextualreview data\n",
        "vocab_size = 5000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "print(x_train[0])\n",
        "# Getting all the wordsfrom word_index dictionary\n",
        "word_idx = imdb.get_word_index()\n",
        "# Originally the index number of a value and not a key,\n",
        "# hence converting the index as key and the words as values\n",
        "word_idx = {i: word for word, i in word_idx.items()}\n",
        "# again printing the review\n",
        "print([word_idx[i] for i in x_train[0]])\n",
        "# Get the minimum and the maximum length of reviews\n",
        "print(\"Max length of a review:: \", len(max((x_train+x_test), key=len)))\n",
        "print(\"Min length of a review:: \", len(min((x_train+x_test), key=len)))\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_words = 400\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
        "x_valid, y_valid = x_train[:64], y_train[:64]\n",
        "x_train_, y_train_ = x_train[64:], y_train[64:]\n",
        "# fixing every word's embedding size to be 32\n",
        "embd_len = 32\n",
        "# Creating a RNN model\n",
        "RNN_model = Sequential(name=\"Simple_RNN\")\n",
        "RNN_model.add(Embedding(vocab_size,\n",
        "embd_len,\n",
        "input_length=max_words))\n",
        "# In case of a stacked(more than one layer of RNN)\n",
        "# use return_sequences=True\n",
        "RNN_model.add(SimpleRNN(128,\n",
        "activation='tanh',\n",
        "return_sequences=False))\n",
        "RNN_model.add(Dense(1, activation='sigmoid'))\n",
        "# printing model summary\n",
        "print(RNN_model.summary())\n",
        "# Compiling model\n",
        "RNN_model.compile(\n",
        "loss=\"binary_crossentropy\",\n",
        "optimizer='adam',\n",
        "metrics=['accuracy']\n",
        ")\n",
        "history = RNN_model.fit(x_train_, y_train_,batch_size=64,\n",
        "epochs=5,\n",
        "verbose=1,\n",
        "validation_data=(x_valid, y_valid))\n",
        "# Printing model score on test data\n",
        "print()\n",
        "print(\"Simple_RNN Score---> \", RNN_model.evaluate(x_test, y_test, verbose=0))\n",
        "plt.title('model_accuracy')\n",
        "plt.ylabel('accuracy') # Corrected the typo from ylable to ylabel\n",
        "plt.xlabel('apoch') # Corrected the typo from xlable to xlabel\n",
        "plt.legend(['train','val'],loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JwDBQ2iyEPeE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}